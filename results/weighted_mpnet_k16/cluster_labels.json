{
  "Cluster 8": {
    "summary": "The posts asked for practical help with consumer-facing AI tools and apps, including how to obtain Sora 2 access or referral codes, whether the tool was region-restricted, and how to disable or change in-app prompts. They also requested and tested specific prompt constructions and outputs—patent and resume prompts, an instruction to force a formal response plus a seahorse emoji, strict color requirements for generated images—and noted odd or humorous model behaviors and simple containment/automation ideas.",
    "name": "Consumer AI access, prompts, and usage help"
  },
  "Cluster 11": {
    "summary": "Posts reported that recent app and model updates had caused broken or duplicated UI controls, sensory‑unfriendly Android layouts, lost or risky save data, failed integrations (Outlook calendar, image generation on the free tier), inconsistent access to custom models, unexpected billing timing, and newly tightened content restrictions (for example around public‑figure likenesses and sexual topics). Users sought confirmation of these regressions, shared a specific workaround for missing models (opening the '+' chat menu or using the auto‑model option with Legacy Models enabled and a Plus/Pro subscription), and expressed frustration that the assistant asked repeatedly for clarifying questions instead of completing requested actions.",
    "name": "ChatGPT app regressions, UI bugs and access issues"
  },
  "Cluster 3": {
    "summary": "Posts described brief anecdotes and reactions to unexpected LLM behavior and outputs, including amusing hallucinations (e.g., misidentifying JD Vance as Kamala Harris), incorrect self-reports of age, untranslated Chinese characters, and noticeable lag. People expressed surprise, amusement, or emotional responses (goosebumps, becoming addicted to a generated story, using it to make a Halloween playlist) and asked short community or account questions, rather than seeking deep technical troubleshooting, access help, or prompt‑engineering strategies.",
    "name": "LLM hallucinations, bugs, and user reactions"
  },
  "Cluster 7": {
    "summary": "Posts argued that rapid LLM advancement and industry trends were creating systemic economic, epistemic, privacy, and safety problems, ranging from a potential AI funding bubble and fears of human economic irrelevance to model failures like memory loss, LCR-induced suppression, and bias amplification. They described emergent behaviors and experiments—consistent person-specific perceptions and anthropomorphic empathetic replies, AI-only simulated civilizations, improving coding tools, and user reports of degraded paid services—that illustrated both growing capabilities and concerning side effects.",
    "name": "Systemic risks and emergent behaviors of LLMs"
  },
  "Cluster 2": {
    "summary": "Posts documented LLMs producing surprising or inconsistent outputs — including unexpected language switches, false or flip-flopped factual answers, repetitive loops, overly cautious guardrails, socialized responses (e.g., \"bless you\"), and creative/personality-rich outputs like fursonas or AI-generated art. Users shared screenshots and anecdotes to question, test, or provoke those behaviors, reported frustration with repetitiveness or sensitivity, and sometimes invited others to help reproduce or evaluate model quirks (for example when testing Claude or visual-only exchanges).",
    "name": "Unexpected LLM responses and behavioral inconsistencies"
  },
  "Cluster 13": {
    "summary": "Users reported and investigated practical operational issues and surprising behaviors of GPT models, posting bug reports and troubleshooting questions about version- and mode-specific inconsistencies, lost draft recovery, document-generation failures, and models referencing outdated uploaded files. They also asked concrete questions about long-term memory and custom‑GPT persistence, compared outputs across Auto/4o/5 and Extended Thinking modes, and shared prompts mainly to probe model capabilities rather than to request access or general creative advice.",
    "name": "GPT bugs, version inconsistencies, and memory troubleshooting"
  },
  "Cluster 4": {
    "summary": "Posts featured creative, aesthetic, and personal content generated by or about language models and AI agents, including poetic name analyses, imagined interviews about religious figures, image-based art titled \"Kin,\" and literary reflections on figures like Hannibal Lecter. They also recorded user reactions and highlighted model limitations, including complaints about hallucinated quotes and copyright refusals, humorous critiques of bizarre image outputs, niche editorial campaigns (like banning the long dash), and apocalyptic job‑loss worries about AI's social impact.",
    "name": "Artistic AI outputs and personal reactions"
  },
  "Cluster 6": {
    "summary": "Users reported that newly tightened AI guardrails and content filters were preventing legitimate uses—blocking access to uploaded files, rejecting innocuous prompts, causing unexpected bans, and prompting subscription cancellations. They expressed confusion about inconsistent policy distinctions (for example allowing depictions of violence while banning sexual content), raised privacy and subscription-related concerns, and shared practical tips for reframing prompts or other workarounds to evade the filters.",
    "name": "Guardrails blocking research, creative writing, and uploads"
  },
  "Cluster 12": {
    "summary": "Posts reported unexpected, erroneous, or surprising behaviors in LLMs and adjacent AI tools, including a recently resolved software bug, altered outputs when extraneous images were attached, missing voice-accent options, strange TTS noises, suspected ad insertion, and sudden removal of contentious information. They also described limitations and misuse vectors such as slow/free image-generation caps, easy scamming of platform bots, AI-generated offensive language or insults that might have enabled cyberbullying, and asked whether these behaviors reflected platform moderation or technical faults.",
    "name": "Unexpected AI behavior, content removal, and misuse reports"
  },
  "Cluster 15": {
    "summary": "Posts described everyday users reporting practical problems, behavior quirks, and usage limits of ChatGPT and other LLM services — such as freezing, unwanted model switching, bracketed code outputs, hidden tags, and temporary feature restrictions — and asked whether issues were widespread or how to fix them. They compared models and subscription tiers, requested app-update and feature information, sought workarounds and automation ideas (timesheet extraction, Zillow integration, fitness tracking), and asked for prompts and project suggestions to better leverage Pro/agent capabilities.",
    "name": "ChatGPT and Claude troubleshooting and usage tips"
  },
  "Cluster 16": {
    "summary": "Users reported that ChatGPT conversation histories, saved memories, or project chats were missing, inaccessible, or inconsistently recalled — citing errors after switching models, stopping generations mid-response, using projects on mobile, or refreshing pages, and they asked for recovery steps or workarounds. The posts focused on reproducible bugs and privacy/persistence concerns with chat storage and memory behavior rather than on prompt crafting, model access, or creative use-cases.",
    "name": "Chat history, memory persistence, and project bugs"
  },
  "Cluster 14": {
    "summary": "Users complained that chatbots repeatedly asked unnecessary clarifying questions, ignored saved instructions, fabricated plausible-sounding answers, or refused to complete tasks and produced bizarre outputs, wasting time and free usage. They asked for concrete prompts, self-audit or debugging techniques to force obedience or transparency, ways to compare or control model \"personalities,\" and speculated about RL reward or token-management causes for silent code failures.",
    "name": "Prompt engineering to stop follow‑ups and hallucinations"
  },
  "Cluster 9": {
    "summary": "Posts compared ChatGPT to other LLMs (notably Grok and local models) and reported that users found it weaker at long‑term memory, roleplay, creative writing, and conversational consistency, which drove many to prefer \"Thinking\" mode or alternative models for sustained context. They also described product and UI behaviors—hidden system prompts shaping tone, the ChatGPT Pulse brief, mid‑task upgrade prompts, and contested GPT‑5 capability claims—that felt opaque or overstated, and they expressed skepticism about marketing versus real‑world storytelling and memory improvements.",
    "name": "ChatGPT memory, modes, and model comparisons"
  },
  "Cluster 1": {
    "summary": "Users compared and ranked multiple LLMs, praising specific versions (for example Sonnet 4.5, Gemini Pro, 4.1, and 4o) and recommending cost-effective alternatives while criticizing regressions, bloat, or new limitations in others (for example complaints about Claude Code). They reported platform and policy changes—older models being removed from ChatGPT Plus, unexpected automatic switches to newer models, UI updates for explicit model selection, and large increases in data retention—that caused frustration around subscription renewals and prompted requests for fixes.",
    "name": "Model preference, forced switching, and ToS changes"
  },
  "Cluster 10": {
    "summary": "Users criticized OpenAI's recent actions — including Sora 2's controversial launch, alleged racial bias, data-retention and privacy shifts, subpoenas, leadership departures, and reduced model-selection convenience — and said these moves eroded trust and prompted some to consider canceling subscriptions. They also sought practical responses by comparing multi-LLM providers, debating local hosting versus cloud (upgrading RAM versus paying for services), asking about listing apps in the ChatGPT store, and exploring open-source competitors and cost-saving strategies.",
    "name": "Criticism of OpenAI policies and migration to alternatives"
  },
  "Cluster 5": {
    "summary": "Posters described using ChatGPT as a confidant and creative outlet for journaling and coping with trauma, PTSD, OCD, fitness and social awkwardness, and shared non‑therapeutic self‑regulation frameworks or personifications to manage its limitations. They reported privacy worries about sharing personal details, frustration with the model's tone and safety‑driven censorship—labeling it creepy, evasive, manipulative, or ableist—and asked for strategies to reduce reliance or steer the chatbot back to ordinary conversation.",
    "name": "ChatGPT as confidant; tone, safety, and privacy concerns"
  }
}